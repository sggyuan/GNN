{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFwPcEWcpfn1NrvJ9FRww1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sggyuan/GNN/blob/main/simplegdata%E8%BD%AC%E6%8D%A2%E7%89%88%E6%9C%AC%E8%BE%93%E5%85%A5vgae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRkpYii8NVPF",
        "outputId": "24b1e634-0641-4a27-8e6f-eb8add4f160c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0+cu121\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import train_test_split_edges"
      ],
      "metadata": {
        "id": "2nxOacBmNYtf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZJQCbK0nY38",
        "outputId": "670d8530-2524-48d1-83b8-134c1aaff536"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data as PyGData\n",
        "\n",
        "# def SimpleGraphData\n",
        "class SimpleGraphData:\n",
        "    def __init__(self, x, edge_index, y=None):\n",
        "        self.x = x  # node feature\n",
        "        self.edge_index = edge_index  # edge index\n",
        "        self.y = y  # node label\n",
        "        self.num_nodes = x.size(0)\n",
        "        self.num_edges = edge_index.size(1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'SimpleGraphData(num_nodes={self.num_nodes}, num_edges={self.num_edges})'\n",
        "\n",
        "# load data\n",
        "data = torch.load('/content/drive/MyDrive/my_graph_data-4.pt')\n",
        "\n",
        "print(\"Loaded data:\", data)\n",
        "\n",
        "# transfer SimpleGraphData to PyTorch Geometric Data\n",
        "pyg_data = PyGData(\n",
        "    x=data.x,\n",
        "    edge_index=data.edge_index,\n",
        "    y=data.y if hasattr(data, 'y') else None\n",
        ")\n",
        "\n",
        "# Remove the mask if it exists\n",
        "pyg_data.train_mask = pyg_data.val_mask = pyg_data.test_mask = None\n",
        "\n",
        "print(\"Converted PyG data:\", pyg_data)\n",
        "\n",
        "# RandomLinkSplit\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "\n",
        "transform = RandomLinkSplit(\n",
        "    num_val=0.05,\n",
        "    num_test=0.1,\n",
        "    is_undirected=True,\n",
        "    add_negative_train_samples=False,\n",
        ")\n",
        "\n",
        "train_data, val_data, test_data = transform(pyg_data)\n",
        "\n",
        "print(\"Train data:\", train_data)\n",
        "print(\"Val data:\", val_data)\n",
        "print(\"Test data:\", test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dDTRp34RbGx",
        "outputId": "0d6d144e-f6da-4e38-f876-3fe770126870"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data: SimpleGraphData(num_nodes=11241, num_edges=80252)\n",
            "Converted PyG data: Data(x=[11241, 20], edge_index=[2, 80252], y=[100000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3d33b5c9cd54>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load('/content/drive/MyDrive/my_graph_data-4.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: Data(x=[11241, 20], edge_index=[2, 68218], y=[100000], edge_label=[34109], edge_label_index=[2, 34109])\n",
            "Val data: Data(x=[11241, 20], edge_index=[2, 68218], y=[100000], edge_label=[4012], edge_label_index=[2, 4012])\n",
            "Test data: Data(x=[11241, 20], edge_index=[2, 72230], y=[100000], edge_label=[8024], edge_label_index=[2, 8024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35y4Yk10Nqgd",
        "outputId": "fe636b93-5346-4d2e-f2f3-896f5d5c9188"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleGraphData(num_nodes=11241, num_edges=80252)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels, cached=True) # cached only for transductive learning\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)"
      ],
      "metadata": {
        "id": "lCpsTz4aNtXS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GAE\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "eQYSSbnKNvaI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_channels = 2\n",
        "num_features = pyg_data.num_features\n",
        "epochs = 100\n",
        "\n",
        "model = GAE(GCNEncoder(num_features, out_channels))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "x = train_data.x.to(device)\n",
        "train_edge_index = train_data.edge_index.to(device)\n",
        "train_edge_label_index = train_data.edge_label_index.to(device)\n",
        "\n",
        "# inizialize the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=10, factor=0.5)"
      ],
      "metadata": {
        "id": "vp2fua8eguFJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(x, train_edge_index)\n",
        "    loss = model.recon_loss(z, train_edge_label_index)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test(pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, train_edge_index)\n",
        "    return model.test(z, pos_edge_index, neg_edge_index)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5t93TrRtN3mx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_val_auc = 0\n",
        "patience = 20\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train()\n",
        "    auc, ap = test(val_data.edge_label_index[:, val_data.edge_label == 1],\n",
        "                   val_data.edge_label_index[:, val_data.edge_label == 0])\n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, AUC: {auc:.4f}, AP: {ap:.4f}')\n",
        "\n",
        "    scheduler.step(auc)  # 根据验证集AUC调整学习率\n",
        "\n",
        "    if auc > best_val_auc:\n",
        "        best_val_auc = auc\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "# 加载最佳模型进行测试\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "test_auc, test_ap = test(test_data.edge_label_index[:, test_data.edge_label == 1],\n",
        "                         test_data.edge_label_index[:, test_data.edge_label == 0])\n",
        "print(f'Final Test AUC: {test_auc:.4f}, AP: {test_ap:.4f}')"
      ],
      "metadata": {
        "id": "0fFT9YUAigOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8145383c-6ce6-4035-9ab2-86b6d55cd2f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 26.9329, AUC: 0.6217, AP: 0.5702\n",
            "Epoch: 002, Loss: 27.0666, AUC: 0.6392, AP: 0.5820\n",
            "Epoch: 003, Loss: 26.2680, AUC: 0.6564, AP: 0.5941\n",
            "Epoch: 004, Loss: 25.2898, AUC: 0.6722, AP: 0.6059\n",
            "Epoch: 005, Loss: 24.3855, AUC: 0.6925, AP: 0.6221\n",
            "Epoch: 006, Loss: 22.2078, AUC: 0.7060, AP: 0.6333\n",
            "Epoch: 007, Loss: 20.0024, AUC: 0.7210, AP: 0.6423\n",
            "Epoch: 008, Loss: 20.4213, AUC: 0.7284, AP: 0.6483\n",
            "Epoch: 009, Loss: 19.9113, AUC: 0.7484, AP: 0.6659\n",
            "Epoch: 010, Loss: 18.1974, AUC: 0.7627, AP: 0.6794\n",
            "Epoch: 011, Loss: 16.2679, AUC: 0.7594, AP: 0.6850\n",
            "Epoch: 012, Loss: 15.4764, AUC: 0.7739, AP: 0.7007\n",
            "Epoch: 013, Loss: 14.7872, AUC: 0.7866, AP: 0.7166\n",
            "Epoch: 014, Loss: 13.5051, AUC: 0.8003, AP: 0.7350\n",
            "Epoch: 015, Loss: 12.2829, AUC: 0.8093, AP: 0.7510\n",
            "Epoch: 016, Loss: 10.7803, AUC: 0.8212, AP: 0.7698\n",
            "Epoch: 017, Loss: 9.4104, AUC: 0.8274, AP: 0.7830\n",
            "Epoch: 018, Loss: 8.1834, AUC: 0.8312, AP: 0.7944\n",
            "Epoch: 019, Loss: 7.3908, AUC: 0.7966, AP: 0.7824\n",
            "Epoch: 020, Loss: 6.3860, AUC: 0.7339, AP: 0.7477\n",
            "Epoch: 021, Loss: 5.3325, AUC: 0.6692, AP: 0.7030\n",
            "Epoch: 022, Loss: 4.5699, AUC: 0.8015, AP: 0.7865\n",
            "Epoch: 023, Loss: 3.8222, AUC: 0.7425, AP: 0.7308\n",
            "Epoch: 024, Loss: 3.0695, AUC: 0.7162, AP: 0.6984\n",
            "Epoch: 025, Loss: 2.4395, AUC: 0.7269, AP: 0.6922\n",
            "Epoch: 026, Loss: 1.8966, AUC: 0.3530, AP: 0.4507\n",
            "Epoch: 027, Loss: 1.5737, AUC: 0.3473, AP: 0.4435\n",
            "Epoch: 028, Loss: 1.5474, AUC: 0.3369, AP: 0.4331\n",
            "Epoch: 029, Loss: 1.5188, AUC: 0.3321, AP: 0.4289\n",
            "Epoch: 030, Loss: 1.5036, AUC: 0.3319, AP: 0.4282\n",
            "Epoch: 031, Loss: 1.4831, AUC: 0.3319, AP: 0.4280\n",
            "Epoch: 032, Loss: 1.4867, AUC: 0.3312, AP: 0.4272\n",
            "Epoch: 033, Loss: 1.4764, AUC: 0.3308, AP: 0.4266\n",
            "Epoch: 034, Loss: 1.4857, AUC: 0.3311, AP: 0.4275\n",
            "Epoch: 035, Loss: 1.4628, AUC: 0.3666, AP: 0.4549\n",
            "Epoch: 036, Loss: 1.4542, AUC: 0.7097, AP: 0.6513\n",
            "Epoch: 037, Loss: 1.4645, AUC: 0.7109, AP: 0.6541\n",
            "Epoch: 038, Loss: 1.4647, AUC: 0.7108, AP: 0.6535\n",
            "Early stopping\n",
            "Final Test AUC: 0.8283, AP: 0.7936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-f0143e0b83a4>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train()\n",
        "\n",
        "    # Evaluate using the val set\n",
        "    auc, ap = test(val_data.edge_label_index[:, val_data.edge_label == 1],\n",
        "                   val_data.edge_label_index[:, val_data.edge_label == 0])\n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, AUC: {auc:.4f}, AP: {ap:.4f}')\n",
        "\n",
        "# Final evaluation on the test set\n",
        "test_auc, test_ap = test(test_data.edge_label_index[:, test_data.edge_label == 1],\n",
        "                         test_data.edge_label_index[:, test_data.edge_label == 0])\n",
        "print(f'Final Test AUC: {test_auc:.4f}, AP: {test_ap:.4f}')"
      ],
      "metadata": {
        "id": "aWU6cQipjGPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686234f2-ab79-4e64-9b38-0bf3b289ef11"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 7.4827, AUC: 0.8287, AP: 0.7936\n",
            "Epoch: 002, Loss: 7.1678, AUC: 0.8216, AP: 0.7918\n",
            "Epoch: 003, Loss: 6.9744, AUC: 0.8018, AP: 0.7838\n",
            "Epoch: 004, Loss: 6.6599, AUC: 0.7974, AP: 0.7825\n",
            "Epoch: 005, Loss: 6.1592, AUC: 0.7854, AP: 0.7765\n",
            "Epoch: 006, Loss: 6.0224, AUC: 0.7435, AP: 0.7527\n",
            "Epoch: 007, Loss: 5.4504, AUC: 0.7089, AP: 0.7302\n",
            "Epoch: 008, Loss: 4.9433, AUC: 0.6854, AP: 0.7158\n",
            "Epoch: 009, Loss: 4.5722, AUC: 0.6651, AP: 0.7022\n",
            "Epoch: 010, Loss: 4.3157, AUC: 0.6520, AP: 0.6930\n",
            "Epoch: 011, Loss: 4.1145, AUC: 0.6381, AP: 0.6834\n",
            "Epoch: 012, Loss: 3.7496, AUC: 0.6460, AP: 0.6831\n",
            "Epoch: 013, Loss: 3.3492, AUC: 0.6159, AP: 0.6489\n",
            "Epoch: 014, Loss: 3.0815, AUC: 0.5742, AP: 0.6180\n",
            "Epoch: 015, Loss: 2.8902, AUC: 0.4851, AP: 0.5422\n",
            "Epoch: 016, Loss: 2.6058, AUC: 0.4510, AP: 0.5120\n",
            "Epoch: 017, Loss: 2.5041, AUC: 0.4260, AP: 0.4893\n",
            "Epoch: 018, Loss: 2.4355, AUC: 0.4155, AP: 0.4777\n",
            "Epoch: 019, Loss: 2.2875, AUC: 0.4083, AP: 0.4714\n",
            "Epoch: 020, Loss: 2.2447, AUC: 0.4113, AP: 0.4737\n",
            "Epoch: 021, Loss: 2.1385, AUC: 0.4206, AP: 0.4819\n",
            "Epoch: 022, Loss: 2.1560, AUC: 0.4332, AP: 0.4933\n",
            "Epoch: 023, Loss: 2.0539, AUC: 0.4667, AP: 0.5210\n",
            "Epoch: 024, Loss: 2.0484, AUC: 0.5049, AP: 0.5531\n",
            "Epoch: 025, Loss: 1.9744, AUC: 0.5220, AP: 0.5637\n",
            "Epoch: 026, Loss: 1.9074, AUC: 0.4721, AP: 0.5306\n",
            "Epoch: 027, Loss: 1.7918, AUC: 0.4088, AP: 0.4835\n",
            "Epoch: 028, Loss: 1.6184, AUC: 0.3959, AP: 0.4785\n",
            "Epoch: 029, Loss: 1.5002, AUC: 0.3961, AP: 0.4793\n",
            "Epoch: 030, Loss: 1.4923, AUC: 0.3725, AP: 0.4593\n",
            "Epoch: 031, Loss: 1.4747, AUC: 0.3728, AP: 0.4594\n",
            "Epoch: 032, Loss: 1.4888, AUC: 0.3919, AP: 0.4747\n",
            "Epoch: 033, Loss: 1.4657, AUC: 0.4146, AP: 0.4934\n",
            "Epoch: 034, Loss: 1.4783, AUC: 0.4343, AP: 0.5083\n",
            "Epoch: 035, Loss: 1.4453, AUC: 0.4759, AP: 0.5387\n",
            "Epoch: 036, Loss: 1.4507, AUC: 0.5001, AP: 0.5563\n",
            "Epoch: 037, Loss: 1.4761, AUC: 0.5477, AP: 0.5867\n",
            "Epoch: 038, Loss: 1.4412, AUC: 0.5550, AP: 0.5910\n",
            "Epoch: 039, Loss: 1.4366, AUC: 0.5439, AP: 0.5842\n",
            "Epoch: 040, Loss: 1.4735, AUC: 0.5270, AP: 0.5720\n",
            "Epoch: 041, Loss: 1.4590, AUC: 0.5267, AP: 0.5719\n",
            "Epoch: 042, Loss: 1.4382, AUC: 0.4760, AP: 0.5351\n",
            "Epoch: 043, Loss: 1.4319, AUC: 0.4752, AP: 0.5346\n",
            "Epoch: 044, Loss: 1.4457, AUC: 0.4839, AP: 0.5412\n",
            "Epoch: 045, Loss: 1.4465, AUC: 0.4880, AP: 0.5411\n",
            "Epoch: 046, Loss: 1.4429, AUC: 0.5139, AP: 0.5588\n",
            "Epoch: 047, Loss: 1.4505, AUC: 0.5711, AP: 0.5964\n",
            "Epoch: 048, Loss: 1.4357, AUC: 0.6266, AP: 0.6327\n",
            "Epoch: 049, Loss: 1.4288, AUC: 0.6273, AP: 0.6321\n",
            "Epoch: 050, Loss: 1.4323, AUC: 0.6307, AP: 0.6326\n",
            "Epoch: 051, Loss: 1.4352, AUC: 0.6309, AP: 0.6318\n",
            "Epoch: 052, Loss: 1.4305, AUC: 0.6091, AP: 0.6108\n",
            "Epoch: 053, Loss: 1.4400, AUC: 0.6088, AP: 0.6110\n",
            "Epoch: 054, Loss: 1.4282, AUC: 0.6059, AP: 0.6101\n",
            "Epoch: 055, Loss: 1.4349, AUC: 0.6052, AP: 0.6101\n",
            "Epoch: 056, Loss: 1.4203, AUC: 0.6055, AP: 0.6107\n",
            "Epoch: 057, Loss: 1.4305, AUC: 0.6050, AP: 0.6104\n",
            "Epoch: 058, Loss: 1.4276, AUC: 0.6050, AP: 0.6105\n",
            "Epoch: 059, Loss: 1.4147, AUC: 0.6045, AP: 0.6098\n",
            "Epoch: 060, Loss: 1.4285, AUC: 0.6043, AP: 0.6092\n",
            "Epoch: 061, Loss: 1.4360, AUC: 0.6038, AP: 0.6083\n",
            "Epoch: 062, Loss: 1.4327, AUC: 0.6048, AP: 0.6086\n",
            "Epoch: 063, Loss: 1.4326, AUC: 0.6078, AP: 0.6096\n",
            "Epoch: 064, Loss: 1.4205, AUC: 0.6083, AP: 0.6097\n",
            "Epoch: 065, Loss: 1.4349, AUC: 0.6083, AP: 0.6096\n",
            "Epoch: 066, Loss: 1.4138, AUC: 0.6084, AP: 0.6101\n",
            "Epoch: 067, Loss: 1.4325, AUC: 0.6079, AP: 0.6101\n",
            "Epoch: 068, Loss: 1.4317, AUC: 0.6080, AP: 0.6106\n",
            "Epoch: 069, Loss: 1.4317, AUC: 0.6040, AP: 0.6088\n",
            "Epoch: 070, Loss: 1.4338, AUC: 0.6044, AP: 0.6093\n",
            "Epoch: 071, Loss: 1.4219, AUC: 0.6042, AP: 0.6093\n",
            "Epoch: 072, Loss: 1.4323, AUC: 0.6017, AP: 0.6069\n",
            "Epoch: 073, Loss: 1.4357, AUC: 0.6018, AP: 0.6073\n",
            "Epoch: 074, Loss: 1.4560, AUC: 0.6018, AP: 0.6071\n",
            "Epoch: 075, Loss: 1.4263, AUC: 0.6047, AP: 0.6095\n",
            "Epoch: 076, Loss: 1.4236, AUC: 0.6093, AP: 0.6120\n",
            "Epoch: 077, Loss: 1.4291, AUC: 0.6086, AP: 0.6119\n",
            "Epoch: 078, Loss: 1.4260, AUC: 0.6080, AP: 0.6110\n",
            "Epoch: 079, Loss: 1.4246, AUC: 0.6079, AP: 0.6105\n",
            "Epoch: 080, Loss: 1.4489, AUC: 0.6078, AP: 0.6101\n",
            "Epoch: 081, Loss: 1.4243, AUC: 0.6079, AP: 0.6106\n",
            "Epoch: 082, Loss: 1.4383, AUC: 0.6304, AP: 0.6326\n",
            "Epoch: 083, Loss: 1.4256, AUC: 0.6316, AP: 0.6334\n",
            "Epoch: 084, Loss: 1.4140, AUC: 0.6274, AP: 0.6309\n",
            "Epoch: 085, Loss: 1.4255, AUC: 0.6295, AP: 0.6336\n",
            "Epoch: 086, Loss: 1.4257, AUC: 0.6292, AP: 0.6336\n",
            "Epoch: 087, Loss: 1.4342, AUC: 0.6296, AP: 0.6337\n",
            "Epoch: 088, Loss: 1.4425, AUC: 0.6296, AP: 0.6337\n",
            "Epoch: 089, Loss: 1.4481, AUC: 0.6295, AP: 0.6334\n",
            "Epoch: 090, Loss: 1.4258, AUC: 0.6301, AP: 0.6341\n",
            "Epoch: 091, Loss: 1.4338, AUC: 0.6305, AP: 0.6345\n",
            "Epoch: 092, Loss: 1.4503, AUC: 0.6304, AP: 0.6342\n",
            "Epoch: 093, Loss: 1.4399, AUC: 0.6303, AP: 0.6341\n",
            "Epoch: 094, Loss: 1.4301, AUC: 0.6304, AP: 0.6343\n",
            "Epoch: 095, Loss: 1.4346, AUC: 0.6291, AP: 0.6335\n",
            "Epoch: 096, Loss: 1.4278, AUC: 0.6294, AP: 0.6335\n",
            "Epoch: 097, Loss: 1.4193, AUC: 0.6295, AP: 0.6342\n",
            "Epoch: 098, Loss: 1.4406, AUC: 0.6301, AP: 0.6353\n",
            "Epoch: 099, Loss: 1.4509, AUC: 0.6302, AP: 0.6357\n",
            "Epoch: 100, Loss: 1.4500, AUC: 0.6302, AP: 0.6359\n",
            "Final Test AUC: 0.6129, AP: 0.6227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zydTbQOjGVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1iseN1GqjGXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import VGAE"
      ],
      "metadata": {
        "id": "cx8myQJejGZ0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalGCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(VariationalGCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
        "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
        "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
      ],
      "metadata": {
        "id": "WVdapYbJjGcL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_channels = 2\n",
        "num_features = pyg_data.num_features\n",
        "epochs = 300\n",
        "\n",
        "model = VGAE(VariationalGCNEncoder(num_features, out_channels))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "x = train_data.x.to(device)\n",
        "train_edge_index = train_data.edge_index.to(device)\n",
        "train_edge_label_index = train_data.edge_label_index.to(device)\n",
        "\n",
        "# inizialize the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "ZRZqCS6snrI3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(x, train_edge_index)\n",
        "    loss = model.recon_loss(z, train_data.edge_label_index)\n",
        "    loss = loss + (1 / train_data.num_nodes) * model.kl_loss()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "def test(pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, train_edge_index)\n",
        "    return model.test(z, pos_edge_index, neg_edge_index)"
      ],
      "metadata": {
        "id": "oXEOHD6oVxqm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/VGAE_experiment_2d_300_epochs')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train()\n",
        "\n",
        "\n",
        "    val_pos_edges = val_data.edge_label_index[:, val_data.edge_label == 1]\n",
        "    val_neg_edges = val_data.edge_label_index[:, val_data.edge_label == 0]\n",
        "    auc, ap = test(val_pos_edges, val_neg_edges)\n",
        "\n",
        "    print('Epoch: {:03d}, Loss: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))\n",
        "\n",
        "    writer.add_scalar('loss', loss, epoch)\n",
        "    writer.add_scalar('auc_val', auc, epoch)\n",
        "    writer.add_scalar('ap_val', ap, epoch)\n",
        "\n",
        "\n",
        "test_pos_edges = test_data.edge_label_index[:, test_data.edge_label == 1]\n",
        "test_neg_edges = test_data.edge_label_index[:, test_data.edge_label == 0]\n",
        "test_auc, test_ap = test(test_pos_edges, test_neg_edges)\n",
        "print('Final Test AUC: {:.4f}, AP: {:.4f}'.format(test_auc, test_ap))\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzVEn8U1j8Dl",
        "outputId": "8ff4283c-8f24-437b-dce9-82ae7913e5ac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 22499.4727, AUC: 0.5077, AP: 0.5039\n",
            "Epoch: 002, Loss: 22145.8027, AUC: 0.5090, AP: 0.5045\n",
            "Epoch: 003, Loss: 21358.6484, AUC: 0.5112, AP: 0.5057\n",
            "Epoch: 004, Loss: 20017.2324, AUC: 0.5123, AP: 0.5084\n",
            "Epoch: 005, Loss: 19852.4082, AUC: 0.5121, AP: 0.5107\n",
            "Epoch: 006, Loss: 19667.0859, AUC: 0.5089, AP: 0.5099\n",
            "Epoch: 007, Loss: 19525.8320, AUC: 0.5106, AP: 0.5116\n",
            "Epoch: 008, Loss: 19373.1367, AUC: 0.5115, AP: 0.5124\n",
            "Epoch: 009, Loss: 19207.8516, AUC: 0.5119, AP: 0.5131\n",
            "Epoch: 010, Loss: 19056.6465, AUC: 0.5139, AP: 0.5144\n",
            "Epoch: 011, Loss: 18897.2754, AUC: 0.5175, AP: 0.5164\n",
            "Epoch: 012, Loss: 18730.6504, AUC: 0.5202, AP: 0.5185\n",
            "Epoch: 013, Loss: 18554.7422, AUC: 0.5234, AP: 0.5211\n",
            "Epoch: 014, Loss: 18326.1836, AUC: 0.5278, AP: 0.5243\n",
            "Epoch: 015, Loss: 18159.4766, AUC: 0.5465, AP: 0.5357\n",
            "Epoch: 016, Loss: 17976.4395, AUC: 0.5512, AP: 0.5390\n",
            "Epoch: 017, Loss: 17776.7793, AUC: 0.5547, AP: 0.5414\n",
            "Epoch: 018, Loss: 17524.7227, AUC: 0.5601, AP: 0.5452\n",
            "Epoch: 019, Loss: 17274.0625, AUC: 0.5654, AP: 0.5493\n",
            "Epoch: 020, Loss: 16991.8984, AUC: 0.5710, AP: 0.5542\n",
            "Epoch: 021, Loss: 16715.8691, AUC: 0.5742, AP: 0.5575\n",
            "Epoch: 022, Loss: 16444.6836, AUC: 0.5798, AP: 0.5627\n",
            "Epoch: 023, Loss: 15965.3838, AUC: 0.5858, AP: 0.5690\n",
            "Epoch: 024, Loss: 15463.8799, AUC: 0.5923, AP: 0.5767\n",
            "Epoch: 025, Loss: 14998.9473, AUC: 0.6006, AP: 0.5850\n",
            "Epoch: 026, Loss: 14666.6738, AUC: 0.6066, AP: 0.5924\n",
            "Epoch: 027, Loss: 14205.2725, AUC: 0.6147, AP: 0.6007\n",
            "Epoch: 028, Loss: 13806.1895, AUC: 0.6229, AP: 0.6101\n",
            "Epoch: 029, Loss: 13224.4404, AUC: 0.6391, AP: 0.6284\n",
            "Epoch: 030, Loss: 12657.7266, AUC: 0.6468, AP: 0.6396\n",
            "Epoch: 031, Loss: 11855.3994, AUC: 0.6652, AP: 0.6609\n",
            "Epoch: 032, Loss: 11035.7197, AUC: 0.6798, AP: 0.6817\n",
            "Epoch: 033, Loss: 10002.1973, AUC: 0.6994, AP: 0.7082\n",
            "Epoch: 034, Loss: 8705.5645, AUC: 0.7108, AP: 0.7289\n",
            "Epoch: 035, Loss: 7099.2363, AUC: 0.6598, AP: 0.7047\n",
            "Epoch: 036, Loss: 4939.9668, AUC: 0.5592, AP: 0.6224\n",
            "Epoch: 037, Loss: 2974.0261, AUC: 0.4645, AP: 0.5267\n",
            "Epoch: 038, Loss: 1501.9362, AUC: 0.4319, AP: 0.4712\n",
            "Epoch: 039, Loss: 599.2483, AUC: 0.4223, AP: 0.4611\n",
            "Epoch: 040, Loss: 155.4023, AUC: 0.4740, AP: 0.4877\n",
            "Epoch: 041, Loss: 28.3829, AUC: 0.4990, AP: 0.4995\n",
            "Epoch: 042, Loss: 1.6487, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 043, Loss: 1.6418, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 044, Loss: 1.6123, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 045, Loss: 1.6180, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 046, Loss: 1.5763, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 047, Loss: 1.5855, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 048, Loss: 1.5934, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 049, Loss: 1.5793, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 050, Loss: 1.5848, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 051, Loss: 1.5552, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 052, Loss: 1.5519, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 053, Loss: 1.5479, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 054, Loss: 1.5536, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 055, Loss: 1.5352, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 056, Loss: 1.5197, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 057, Loss: 1.5383, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 058, Loss: 1.5289, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 059, Loss: 1.5259, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 060, Loss: 1.5208, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 061, Loss: 1.5102, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 062, Loss: 1.5212, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 063, Loss: 1.5018, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 064, Loss: 1.5051, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 065, Loss: 1.5064, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 066, Loss: 1.4891, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 067, Loss: 1.5087, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 068, Loss: 1.4890, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 069, Loss: 1.4907, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 070, Loss: 1.4921, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 071, Loss: 1.4898, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 072, Loss: 1.4832, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 073, Loss: 1.4819, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 074, Loss: 1.4790, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 075, Loss: 1.4835, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 076, Loss: 1.4804, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 077, Loss: 1.4756, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 078, Loss: 1.4751, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 079, Loss: 1.4700, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 080, Loss: 1.4763, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 081, Loss: 1.4669, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 082, Loss: 1.4707, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 083, Loss: 1.4673, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 084, Loss: 1.4691, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 085, Loss: 1.4732, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 086, Loss: 1.4624, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 087, Loss: 1.4633, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 088, Loss: 1.4591, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 089, Loss: 1.4655, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 090, Loss: 1.4625, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 091, Loss: 1.4679, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 092, Loss: 1.4569, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 093, Loss: 1.4638, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 094, Loss: 1.4631, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 095, Loss: 1.4607, AUC: 0.4970, AP: 0.4994\n",
            "Epoch: 096, Loss: 1.4645, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 097, Loss: 1.4657, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 098, Loss: 1.4544, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 099, Loss: 1.4614, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 100, Loss: 1.4634, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 101, Loss: 1.4590, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 102, Loss: 1.4563, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 103, Loss: 1.4575, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 104, Loss: 1.4596, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 105, Loss: 1.4582, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 106, Loss: 1.4599, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 107, Loss: 1.4565, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 108, Loss: 1.4551, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 109, Loss: 1.4492, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 110, Loss: 1.4547, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 111, Loss: 1.4542, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 112, Loss: 1.4566, AUC: 0.4970, AP: 0.4994\n",
            "Epoch: 113, Loss: 1.4588, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 114, Loss: 1.4596, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 115, Loss: 1.4540, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 116, Loss: 1.4576, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 117, Loss: 1.4538, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 118, Loss: 1.4544, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 119, Loss: 1.4503, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 120, Loss: 1.4521, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 121, Loss: 1.4531, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 122, Loss: 1.4479, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 123, Loss: 1.4526, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 124, Loss: 1.4474, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 125, Loss: 1.4503, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 126, Loss: 1.4537, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 127, Loss: 1.4486, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 128, Loss: 1.4468, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 129, Loss: 1.4471, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 130, Loss: 1.4541, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 131, Loss: 1.4526, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 132, Loss: 1.4572, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 133, Loss: 1.4490, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 134, Loss: 1.4494, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 135, Loss: 1.4465, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 136, Loss: 1.4566, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 137, Loss: 1.4504, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 138, Loss: 1.4470, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 139, Loss: 1.4488, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 140, Loss: 1.4550, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 141, Loss: 1.4526, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 142, Loss: 1.4507, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 143, Loss: 1.4479, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 144, Loss: 1.4482, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 145, Loss: 1.4452, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 146, Loss: 1.4540, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 147, Loss: 1.4500, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 148, Loss: 1.4449, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 149, Loss: 1.4511, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 150, Loss: 1.4447, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 151, Loss: 1.4489, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 152, Loss: 1.4448, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 153, Loss: 1.4597, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 154, Loss: 1.4532, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 155, Loss: 1.4452, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 156, Loss: 1.4495, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 157, Loss: 1.4452, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 158, Loss: 1.4500, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 159, Loss: 1.4435, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 160, Loss: 1.4533, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 161, Loss: 1.4430, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 162, Loss: 1.4445, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 163, Loss: 1.4487, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 164, Loss: 1.4478, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 165, Loss: 1.4499, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 166, Loss: 1.4431, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 167, Loss: 1.4434, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 168, Loss: 1.4435, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 169, Loss: 1.4432, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 170, Loss: 1.4434, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 171, Loss: 1.4509, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 172, Loss: 1.4451, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 173, Loss: 1.4433, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 174, Loss: 1.4535, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 175, Loss: 1.4492, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 176, Loss: 1.4380, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 177, Loss: 1.4452, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 178, Loss: 1.4531, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 179, Loss: 1.4443, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 180, Loss: 1.4473, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 181, Loss: 1.4477, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 182, Loss: 1.4470, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 183, Loss: 1.4470, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 184, Loss: 1.4466, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 185, Loss: 1.4419, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 186, Loss: 1.4408, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 187, Loss: 1.4416, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 188, Loss: 1.4464, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 189, Loss: 1.4380, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 190, Loss: 1.4421, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 191, Loss: 1.4436, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 192, Loss: 1.4383, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 193, Loss: 1.4449, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 194, Loss: 1.4436, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 195, Loss: 1.4397, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 196, Loss: 1.4488, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 197, Loss: 1.4433, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 198, Loss: 1.4422, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 199, Loss: 1.4402, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 200, Loss: 1.4395, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 201, Loss: 1.4351, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 202, Loss: 1.4400, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 203, Loss: 1.4399, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 204, Loss: 1.4399, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 205, Loss: 1.4394, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 206, Loss: 1.4408, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 207, Loss: 1.4408, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 208, Loss: 1.4460, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 209, Loss: 1.4428, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 210, Loss: 1.4398, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 211, Loss: 1.4444, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 212, Loss: 1.4507, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 213, Loss: 1.4422, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 214, Loss: 1.4426, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 215, Loss: 1.4482, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 216, Loss: 1.4407, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 217, Loss: 1.4400, AUC: 0.4985, AP: 0.4997\n",
            "Epoch: 218, Loss: 1.4367, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 219, Loss: 1.4383, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 220, Loss: 1.4384, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 221, Loss: 1.4398, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 222, Loss: 1.4422, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 223, Loss: 1.4402, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 224, Loss: 1.4369, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 225, Loss: 1.4509, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 226, Loss: 1.4423, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 227, Loss: 1.4447, AUC: 0.4970, AP: 0.4991\n",
            "Epoch: 228, Loss: 1.4401, AUC: 0.4975, AP: 0.4992\n",
            "Epoch: 229, Loss: 1.4445, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 230, Loss: 1.4370, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 231, Loss: 1.4399, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 232, Loss: 1.4444, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 233, Loss: 1.4432, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 234, Loss: 1.4392, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 235, Loss: 1.4441, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 236, Loss: 1.4478, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 237, Loss: 1.4411, AUC: 0.4985, AP: 0.4997\n",
            "Epoch: 238, Loss: 1.4430, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 239, Loss: 1.4421, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 240, Loss: 1.4426, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 241, Loss: 1.4369, AUC: 0.4985, AP: 0.4997\n",
            "Epoch: 242, Loss: 1.4393, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 243, Loss: 1.4480, AUC: 0.4985, AP: 0.4997\n",
            "Epoch: 244, Loss: 1.4368, AUC: 0.4985, AP: 0.4997\n",
            "Epoch: 245, Loss: 1.4400, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 246, Loss: 1.4371, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 247, Loss: 1.4434, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 248, Loss: 1.4340, AUC: 0.4995, AP: 0.5000\n",
            "Epoch: 249, Loss: 1.4361, AUC: 0.4985, AP: 0.4997\n",
            "Epoch: 250, Loss: 1.4356, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 251, Loss: 1.4376, AUC: 0.4975, AP: 0.4993\n",
            "Epoch: 252, Loss: 1.4339, AUC: 0.4970, AP: 0.4990\n",
            "Epoch: 253, Loss: 1.4366, AUC: 0.4970, AP: 0.4990\n",
            "Epoch: 254, Loss: 1.4381, AUC: 0.4970, AP: 0.4990\n",
            "Epoch: 255, Loss: 1.4408, AUC: 0.4975, AP: 0.4991\n",
            "Epoch: 256, Loss: 1.4433, AUC: 0.4980, AP: 0.4991\n",
            "Epoch: 257, Loss: 1.4396, AUC: 0.4980, AP: 0.4991\n",
            "Epoch: 258, Loss: 1.4385, AUC: 0.4980, AP: 0.4991\n",
            "Epoch: 259, Loss: 1.4393, AUC: 0.4985, AP: 0.4991\n",
            "Epoch: 260, Loss: 1.4361, AUC: 0.4980, AP: 0.4990\n",
            "Epoch: 261, Loss: 1.4419, AUC: 0.4980, AP: 0.4990\n",
            "Epoch: 262, Loss: 1.4337, AUC: 0.4980, AP: 0.4990\n",
            "Epoch: 263, Loss: 1.4397, AUC: 0.4980, AP: 0.4990\n",
            "Epoch: 264, Loss: 1.4448, AUC: 0.4980, AP: 0.4990\n",
            "Epoch: 265, Loss: 1.4373, AUC: 0.4980, AP: 0.4990\n",
            "Epoch: 266, Loss: 1.4349, AUC: 0.4980, AP: 0.4990\n",
            "Epoch: 267, Loss: 1.4411, AUC: 0.4980, AP: 0.4990\n",
            "Epoch: 268, Loss: 1.4447, AUC: 0.4985, AP: 0.4991\n",
            "Epoch: 269, Loss: 1.4367, AUC: 0.4985, AP: 0.4991\n",
            "Epoch: 270, Loss: 1.4381, AUC: 0.4980, AP: 0.4991\n",
            "Epoch: 271, Loss: 1.4413, AUC: 0.4985, AP: 0.4996\n",
            "Epoch: 272, Loss: 1.4379, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 273, Loss: 1.4409, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 274, Loss: 1.4382, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 275, Loss: 1.4446, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 276, Loss: 1.4362, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 277, Loss: 1.4378, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 278, Loss: 1.4383, AUC: 0.5000, AP: 0.5000\n",
            "Epoch: 279, Loss: 1.4391, AUC: 0.5005, AP: 0.5002\n",
            "Epoch: 280, Loss: 1.4362, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 281, Loss: 1.4396, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 282, Loss: 1.4354, AUC: 0.5010, AP: 0.5007\n",
            "Epoch: 283, Loss: 1.4378, AUC: 0.5015, AP: 0.5010\n",
            "Epoch: 284, Loss: 1.4390, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 285, Loss: 1.4368, AUC: 0.5005, AP: 0.5003\n",
            "Epoch: 286, Loss: 1.4404, AUC: 0.5008, AP: 0.5004\n",
            "Epoch: 287, Loss: 1.4380, AUC: 0.5010, AP: 0.5005\n",
            "Epoch: 288, Loss: 1.4451, AUC: 0.5015, AP: 0.5008\n",
            "Epoch: 289, Loss: 1.4399, AUC: 0.5000, AP: 0.5001\n",
            "Epoch: 290, Loss: 1.4381, AUC: 0.5000, AP: 0.4998\n",
            "Epoch: 291, Loss: 1.4384, AUC: 0.5000, AP: 0.4997\n",
            "Epoch: 292, Loss: 1.4380, AUC: 0.5000, AP: 0.5002\n",
            "Epoch: 293, Loss: 1.4392, AUC: 0.5000, AP: 0.5004\n",
            "Epoch: 294, Loss: 1.4397, AUC: 0.5015, AP: 0.5009\n",
            "Epoch: 295, Loss: 1.4367, AUC: 0.5010, AP: 0.5008\n",
            "Epoch: 296, Loss: 1.4353, AUC: 0.5010, AP: 0.5009\n",
            "Epoch: 297, Loss: 1.4367, AUC: 0.5005, AP: 0.5009\n",
            "Epoch: 298, Loss: 1.4387, AUC: 0.5015, AP: 0.5011\n",
            "Epoch: 299, Loss: 1.4359, AUC: 0.5020, AP: 0.5012\n",
            "Epoch: 300, Loss: 1.4386, AUC: 0.5020, AP: 0.5011\n",
            "Final Test AUC: 0.4993, AP: 0.4999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pn-9DIVvj9kF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer.close()"
      ],
      "metadata": {
        "id": "FCtI2XNOJKqo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "mQWb2lX8OnGX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "-TQ2N0USOpbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaRSuYROHVIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}